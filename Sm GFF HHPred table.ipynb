{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sm GFF HHPred table\n",
    "\n",
    "\n",
    "\n",
    "## Aim\n",
    "\n",
    "GFF annotation might be sometimes misleading when it comes to function prediction (especially if generated from simple blast). In order to complement this annotation, we will use a second method based on similarity detection relying on Hidden Markov Model as implement in HHsearch from the [HH-suite](https://doi.org/10.1093/bioinformatics/bti125). We will analyze the predicted protein sequences from the latest version of the genome, sequences corresponding to each isoform (i.e., transcript) predicted from the genome. We will generate a table with both GFF annotation (from the latest GFF file version) and HHsearch annotation.\n",
    "\n",
    "Historically, this approach was very helpful for our [oxamniquine resistance work](https://doi.org/10.1126/science.1243106) in *Schistosoma mansoni*. The gene involved in drug activation was known to have a sulfotransferase activity from previous studies. Our genetic approach revealed promising candidates which were annotated as epimerases in the v5 genome of *S. mansoni*, but epimerases do not have the expected sulfotransferase activity. However, the candidates turned out to be closer to sulfotransferases than epimerases after using the HHPred server and this was later biochemically confirmed. This convinced us to use this approach to systematically complement GFF annotation.\n",
    "\n",
    "\n",
    "\n",
    "## Environment and databases\n",
    "\n",
    "### Conda environment\n",
    "\n",
    "This allows for a better replication of the analysis and database generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if conda available\n",
    "[[ ! $(which conda 2> /dev/null) ]] && echo \"conda not available in \\$PATH. Please interrupt the kernel and fix the situation.\" && sleep inf\n",
    "\n",
    "# Creating conda environment\n",
    "conda env create -f .env/env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell must be run each time a new session of Jupyter is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the environment\n",
    "source $(sed \"s,/bin/conda,,\" <<<$CONDA_EXE)/etc/profile.d/conda.sh\n",
    "conda activate hhpred\n",
    "\n",
    "# PERL lib (for splitfasta.pl)\n",
    "export PERL5LIB=$CONDA_PREFIX/lib/5.26.2:$CONDA_PREFIX/scripts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download databases\n",
    "\n",
    "**NB:** It can take a very long time (up to 6h) to download and decompress the databases (up to 6h)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_dir=\"db\"\n",
    "[[ ! -d \"$db_dir\" ]] && mkdir -p \"$db_dir\"\n",
    "\n",
    "# Download PDB database\n",
    "wget -P \"$db_dir\" http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/pdb70_from_mmcif_200902.tar.gz\n",
    "tar -C \"$db_dir\" --use-compress-program=pigz -xvf db/pdb70_from_mmcif_200902.tar.gz\n",
    "\n",
    "# Download Pfam database\n",
    "wget -P \"$db_dir\" http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/pfamA_32.0.tar.gz\n",
    "tar -C \"$db_dir\" --use-compress-program=pigz -xvf db/pfamA_32.0.tar.gz\n",
    "\n",
    "# Download SCOP database\n",
    "wget -P \"$db_dir\" http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/scop70_1.75_hhsuite3.tar.gz\n",
    "tar -C \"$db_dir\" --use-compress-program=pigz -xvf db/scop70_1.75_hhsuite3.tar.gz\n",
    "\n",
    "# Allow file read by everyone (otherwise hhsearch will emits an open file error)\n",
    "chmod +r \"$db_dir\"/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and database creation\n",
    "\n",
    "### Generate HHPred database\n",
    "\n",
    "This step downloads the predicted protein sequences from [WomBase ParaSite website](https://parasite.wormbase.org/Schistosoma_mansoni_prjea36577/Info/Index/) (version WBPS14). The sequences are split in individual fasta files. Path to the files is stored into a text file (list) and the list is split in batches of 100 sequences (this can be changed to generate shorter lists in order to speed up the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the protein sequences\n",
    "wget \"ftp://ftp.ebi.ac.uk/pub/databases/wormbase/parasite/releases/WBPS14/species/schistosoma_mansoni/PRJEA36577/schistosoma_mansoni.PRJEA36577.WBPS14.protein.fa.gz\"\n",
    "pigz -d schistosoma_mansoni.PRJEA36577.WBPS14.protein.fa.gz\n",
    "\n",
    "# Split the protein fasta file in individual files\n",
    "[[ ! -d data ]] && mkdir data\n",
    "mv schistosoma_mansoni.PRJEA36577.WBPS14.protein.fa data/\n",
    "cd data\n",
    "perl $CONDA_PREFIX/scripts/splitfasta.pl schistosoma_mansoni.PRJEA36577.WBPS14.protein.fa\n",
    "cd ..\n",
    "mv data/schistosoma_mansoni.PRJEA36577.WBPS14.protein.fa .\n",
    "\n",
    "# Create a list file of fasta paths to process and split the list for parallelization\n",
    "[[ ! -d list.d ]] && mkdir list.d\n",
    "ls -1 data/* > list\n",
    "split -l 100 -a 3 -d list list.d/list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uses the custom `hhpred-ann.sh` script to run `hhsearch` on sequences given by a split list and to generate a corresponding list with the best annotation for each sequence. This is efficient only if the split lists are run in parallel on a computer cluster. In the cell below, each split list is submitted to a node using Oracle Grid Engine. This parallelization helps to speed up tremendously the analysis (run takes ~4h instead of ~400h).\n",
    "\n",
    "The script takes in account all the databases and a minimum probability of 50% to retain the best match as recommended by the [HH-suite documentation](https://github.com/soedinglab/hh-suite/wiki#how-can-i-verify-if-a-database-match-is-homologous). Help on script usage is available using `./hhpred-ann.sh -h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status folder to store the jobs' status\n",
    "[[ ! -d status ]] && mkdir status\n",
    "\n",
    "# Submit split list to the nodes\n",
    "for i in list.d/*\n",
    "do\n",
    "    qsub -V -cwd -o status -j y -r y -S /bin/bash hhpred-ann.sh -i \"$i\" -d db/pdb70 db/scop70_1.75 db/pfam -p 50\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all the sequences have been annotated, this final step concatenates all the annotations in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the database\n",
    "db_hh=hhpred_ann_$(date +%F).db\n",
    "cat results/list.* > \"$db_hh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract annotations from the GFF file.\n",
    "\n",
    "This step downloads the GFF file from WomBase ParaSite website (version WBPS14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GFF\n",
    "wget ftp://ftp.ebi.ac.uk/pub/databases/wormbase/parasite/releases/WBPS14/species/schistosoma_mansoni/PRJEA36577/schistosoma_mansoni.PRJEA36577.WBPS14.annotations.gff3.gz\n",
    "pigz -d schistosoma_mansoni.PRJEA36577.WBPS14.annotations.gff3.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the protein function annotations from the GFF, we use a single command pipeline that executes the following steps:\n",
    "- Extract the info column\n",
    "- Extract the ID and annotation\n",
    "- Clean the information and add NA when needed\n",
    "- Convert the HTML code in to normal characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gff=gff_table.tsv\n",
    "\n",
    "# Extract ID and annotation\n",
    "awk '$3 == \"mRNA\" {print $0}' schistosoma_mansoni.PRJEA36577.WBPS14.annotations.gff3 |\\\n",
    "    cut -f 9 |\\\n",
    "    awk -F ':' '{print $2\"\\t\"$6}' |\\\n",
    "    sed -r \"s/^(.*);.*\\t/\\1\\t/g ; s/ %0A.*//g ; s/\\t$/\\tNA/\" |\\\n",
    "    sed 's@+@ @g;s@%@\\\\x@g' | xargs -P $(nproc) -n1 -d '\\n' printf \"%b\\n\" > gff_table.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge GFF and HHPred annotations.\n",
    "\n",
    "This final step merges both GFF and HHPred annotations in a single file for each isoform (i.e., transcript)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join GFF annotation and HHPred annotation\n",
    "join -t $'\\t' <(sort -k 1 \"$db_gff\") <(sort -k 1 \"$db_hh\") > Sm_transcript_table_gff-hhpred_$(date +%F).tsv \n",
    "\n",
    "# Add header\n",
    "sed -i -r \"1s/^/#Transcript_ID\\tGFF_annotation\\tHHPred_annotation\\n/\" Sm_transcript_table_gff-hhpred_$(date +%F).tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "217px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
